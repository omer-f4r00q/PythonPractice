{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1857eeb2-9a8d-4b7d-84cd-37125de0a530",
   "metadata": {},
   "source": [
    "# **Module 9: The Scientific Computing Stack**\n",
    "# **Part 2: Pandas (Panel Data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0195df4-f08a-4d1c-8842-7a6b937bf737",
   "metadata": {},
   "source": [
    "## **1. Introduce the Concept: The Data Scientist's Spreadsheet**\n",
    "\n",
    "**What is Pandas?**\n",
    "Pandas is the most popular and essential Python library for data manipulation and analysis. It provides data structures and functions designed to make working with **structured, labeled data** intuitive and efficient.\n",
    "\n",
    "**Why does it exist? (The Problem with NumPy Arrays)**\n",
    "NumPy is fantastic for raw numerical computation, but it has limitations for real-world data analysis:\n",
    "1.  **No Labels:** A NumPy array is just a grid of numbers. The rows and columns are identified only by their integer index (`0, 1, 2...`). You can't ask a NumPy array, \"What were the sales for the 'electronics' category in 'February'?\" You can only ask for the value at `[row 1, column 2]`. This is not intuitive.\n",
    "2.  **Homogeneous Data:** NumPy arrays require all elements to be of the same data type. Real-world datasets are almost always a mix of types: text (customer names), numbers (sales), and dates (order date). You can't store this efficiently in a single NumPy array.\n",
    "\n",
    "**The Solution: The Pandas DataFrame and Series**\n",
    "Pandas introduces two new, powerful data structures that solve these problems:\n",
    "\n",
    "1.  **The `Series`:**\n",
    "    *   **What it is:** A one-dimensional, labeled array.\n",
    "    *   **Analogy:** Think of it as a single **column** in a spreadsheet or a table. It has the data itself (which is a NumPy array under the hood) and an associated **index** that labels each data point.\n",
    "\n",
    "2.  **The `DataFrame`:**\n",
    "    *   **What it is:** A two-dimensional, labeled data structure with columns of potentially different types.\n",
    "    *   **Analogy:** This is the main event. A `DataFrame` is your entire **spreadsheet** or **SQL table**. It's a collection of `Series` objects that share a common index. It has both a **row index** and **column labels**.\n",
    "\n",
    "**Visualizing the Relationship:**\n",
    "\n",
    "```\n",
    "      <-- DataFrame (The whole table) -->\n",
    "      \n",
    "      Column 'A'   Column 'B'   Column 'C'\n",
    "      (a Series)   (a Series)   (a Series)\n",
    "      \n",
    "      +------------+------------+------------+\n",
    "Row 0 |     1      |   'hello'  |   True     |\n",
    "      +------------+------------+------------+\n",
    "Row 1 |     2      |   'world'  |   False    |\n",
    "      +------------+------------+------------+\n",
    "Row 2 |     3      |   'pandas' |   True     |\n",
    "      +------------+------------+------------+\n",
    "\n",
    " ^\n",
    " |\n",
    "Index (Labels for the rows)\n",
    "```\n",
    "\n",
    "**Key Takeaway:** Pandas gives us the `DataFrame`, which combines the high-performance numerical computation of NumPy with the flexible, intuitive labeling of a spreadsheet. It is the single most important tool in a data scientist's toolkit for cleaning, transforming, and analyzing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dcc13-44f1-4329-828c-e0751d67dee4",
   "metadata": {},
   "source": [
    "### **2. Simple Examples**\n",
    "\n",
    "By convention, Pandas is always imported with the alias `pd`.\n",
    "##### **Example 1: Creating a Pandas `Series`**\n",
    "\n",
    "A `Series` is like a NumPy array with an explicit index. You can create one from a list.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# A simple Python list of data\n",
    "data = [10, 20, 30, 40]\n",
    "# Custom labels for our index\n",
    "labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "# Create a Series with custom labels\n",
    "my_series = pd.Series(data=data, index=labels)\n",
    "\n",
    "print(\"--- A Pandas Series ---\")\n",
    "print(my_series)\n",
    "\n",
    "# You can access data by its label, like a dictionary\n",
    "print(f\"\\nValue at label 'c': {my_series['c']}\")\n",
    "\n",
    "# If you don't provide an index, Pandas creates a default integer index\n",
    "default_series = pd.Series(data)\n",
    "print(\"\\n--- A Series with a default index ---\")\n",
    "print(default_series)\n",
    "print(f\"\\nValue at index 2: {default_series[2]}\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "--- A Pandas Series ---\n",
    "a    10\n",
    "b    20\n",
    "c    30\n",
    "d    40\n",
    "dtype: int64\n",
    "\n",
    "Value at label 'c': 30\n",
    "\n",
    "--- A Series with a default index ---\n",
    "0    10\n",
    "1    20\n",
    "2    30\n",
    "3    40\n",
    "dtype: int64\n",
    "\n",
    "Value at index 2: 30\n",
    "```\n",
    "*Notice how the `Series` is displayed with its index on the left and the data on the right.*\n",
    "\n",
    "##### **Example 2: Creating a Pandas `DataFrame`**\n",
    "\n",
    "The most common way to create a `DataFrame` is from a Python dictionary where the keys become the column names and the values become the column data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np # Often used together with Pandas\n",
    "\n",
    "# Create a dictionary of data\n",
    "# Note: Each value is a list or NumPy array of the SAME length\n",
    "data_dict = {\n",
    "    'student_name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'score': [85, 92, 78, 88],\n",
    "    'completed_hw': [True, True, False, True]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data_dict) # 'df' is the conventional variable name for a DataFrame\n",
    "\n",
    "print(\"--- A Pandas DataFrame ---\")\n",
    "print(df)\n",
    "\n",
    "# --- Accessing Data ---\n",
    "\n",
    "# 1. Select a single column (this returns a Series!)\n",
    "scores_column = df['score']\n",
    "print(\"\\n--- Selecting the 'score' column (a Series) ---\")\n",
    "print(scores_column)\n",
    "\n",
    "# 2. Select multiple columns\n",
    "# Pass a list of column names\n",
    "subset_df = df[['student_name', 'completed_hw']]\n",
    "print(\"\\n--- Selecting multiple columns (a new DataFrame) ---\")\n",
    "print(subset_df)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "--- A Pandas DataFrame ---\n",
    "  student_name  score  completed_hw\n",
    "0        Alice     85          True\n",
    "1          Bob     92          True\n",
    "2      Charlie     78         False\n",
    "3        David     88          True\n",
    "\n",
    "--- Selecting the 'score' column (a Series) ---\n",
    "0    85\n",
    "1    92\n",
    "2    78\n",
    "3    88\n",
    "Name: score, dtype: int64\n",
    "\n",
    "--- Selecting multiple columns (a new DataFrame) ---\n",
    "  student_name  completed_hw\n",
    "0        Alice          True\n",
    "1          Bob          True\n",
    "2      Charlie         False\n",
    "3        David          True\n",
    "```\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   A `pd.Series` is a 1D labeled array.\n",
    "*   A `pd.DataFrame` is a 2D labeled table.\n",
    "*   You create a `DataFrame` from a dictionary of lists.\n",
    "*   You select columns using square brackets `[]`, similar to how you access values in a dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b508c9a-cfba-4357-900a-22459e5f2fac",
   "metadata": {},
   "source": [
    "### **3. Task**\n",
    "\n",
    "**Goal:** Create a Pandas `DataFrame` to store information about a few countries. Then, practice selecting specific columns from it.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Import Pandas** with its standard alias `pd`.\n",
    "2.  Create a Python **dictionary** that will be used to build the `DataFrame`. The dictionary should contain the following data:\n",
    "    *   A key `'country'` with a list of values: `[\"USA\", \"Canada\", \"Mexico\", \"Brazil\"]`\n",
    "    *   A key `'population_millions'` with a list of values: `[331, 38, 126, 212]`\n",
    "    *   A key `'continent'` with a list of values: `[\"North America\", \"North America\", \"North America\", \"South America\"]`\n",
    "3.  Use this dictionary to create a Pandas `DataFrame` and assign it to a variable named `countries_df`.\n",
    "4.  **Print the entire `countries_df` DataFrame.**\n",
    "5.  After printing the full DataFrame, **select only the `country` and `continent` columns** and print this new, smaller DataFrame.\n",
    "\n",
    "This task will test your ability to structure data in a dictionary and then use it to create and select from a `DataFrame`. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2a7259-6d7a-4b20-9294-3fcf6c57419c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries DataFrame:\n",
      "        country  population_millions      continent\n",
      "0         India                 1400           Asia\n",
      "1  Saudi Arabia                  100    Middle East\n",
      "2         China                 2000           Asia\n",
      "3           USA                  500  North America\n",
      "4        Russia                  300           Asia\n",
      "\n",
      "Smaller DataFrame:\n",
      "        country      continent\n",
      "0         India           Asia\n",
      "1  Saudi Arabia    Middle East\n",
      "2         China           Asia\n",
      "3           USA  North America\n",
      "4        Russia           Asia\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "countries_dict = {\n",
    "    \"country\": [\"India\", \"Saudi Arabia\", \"China\", \"USA\", \"Russia\"],\n",
    "    \"population_millions\": [1400, 100, 2000, 500, 300],\n",
    "    \"continent\": [\"Asia\", \"Middle East\", \"Asia\", \"North America\", \"Asia\"]\n",
    "}\n",
    "countries_df = pd.DataFrame(countries_dict)\n",
    "print(f\"Countries DataFrame:\\n{countries_df}\\n\")\n",
    "\n",
    "print(f\"Smaller DataFrame:\\n{countries_df[[\"country\", \"continent\"]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19178e73-8dfd-42c6-89d9-54ccfa9c1aed",
   "metadata": {},
   "source": [
    "### **Reading Data from a File**\n",
    "\n",
    "While creating DataFrames by hand is useful for small examples, in the real world, 99% of the time you will be **loading data from an external file**, most commonly a CSV file.\n",
    "\n",
    "Pandas makes this incredibly easy with the **`pd.read_csv()`** function. This one function is the primary entry point for almost all data analysis.\n",
    "\n",
    "Let's use the `inventory.csv` file we created in our File I/O module.\n",
    "\n",
    "**File: `inventory.csv`**\n",
    "```csv\n",
    "product_name,price,quantity\n",
    "Laptop,1200.00,8\n",
    "Mouse,25.50,30\n",
    "Keyboard,75.00,15\n",
    "Monitor,300.00,12\n",
    "Webcam,50.00,5\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# The path to our CSV file\n",
    "filename = 'inventory.csv'\n",
    "\n",
    "try:\n",
    "    # This single line does all the work:\n",
    "    # 1. Opens the file.\n",
    "    # 2. Reads the data.\n",
    "    # 3. Uses the first row as the header (column names).\n",
    "    # 4. Infers the data types (e.g., price becomes a float, quantity an int).\n",
    "    # 5. Closes the file.\n",
    "    inventory_df = pd.read_csv(filename)\n",
    "    \n",
    "    print(\"--- Successfully loaded data from CSV into a DataFrame ---\")\n",
    "    print(inventory_df)\n",
    "    \n",
    "    # Let's check the data types that Pandas inferred\n",
    "    print(\"\\n--- Data Types (dtypes) of each column ---\")\n",
    "    print(inventory_df.dtypes)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filename}' was not found.\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "--- Successfully loaded data from CSV into a DataFrame ---\n",
    "  product_name    price  quantity\n",
    "0       Laptop  1200.00         8\n",
    "1        Mouse    25.50        30\n",
    "2     Keyboard    75.00        15\n",
    "3      Monitor   300.00        12\n",
    "4       Webcam    50.00         5\n",
    "\n",
    "--- Data Types (dtypes) of each column ---\n",
    "product_name     object  <-- 'object' is Pandas's term for a string\n",
    "price           float64\n",
    "quantity          int64\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   `pd.read_csv()` is the most important function for getting data into Pandas.\n",
    "*   It's powerful: it handles headers and automatically tries to figure out the best data type for each column (`.dtypes` attribute lets you check this).\n",
    "*   This is far easier and more powerful than the manual `csv` module we learned before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96635fa-5116-46b8-bd51-0421ba8d9683",
   "metadata": {},
   "source": [
    "### ** Inspecting a DataFrame**\n",
    "\n",
    "When a data scientist loads a new dataset, they never just start analyzing it. The very first step is always to get a quick overview of the data. This is like a doctor checking a patient's vital signs before making a diagnosis.\n",
    "\n",
    "Pandas provides several simple and essential methods for this initial inspection.\n",
    "\n",
    "Let's assume we've just loaded our `inventory_df` from the CSV file.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Assume this DataFrame has just been loaded from a large CSV file\n",
    "inventory_df = pd.read_csv('inventory.csv')\n",
    "```\n",
    "\n",
    "Here are the first five commands a data scientist would almost always run:\n",
    "\n",
    "**1. `.head()`: Look at the first few rows**\n",
    "This is the most common command. It shows you the first 5 rows by default, giving you a quick feel for the column names and the type of data in each column.\n",
    "\n",
    "```python\n",
    "# Show the first 5 rows\n",
    "print(\"--- 1. First 5 rows with .head() ---\")\n",
    "print(inventory_df.head())\n",
    "\n",
    "# You can also specify the number of rows\n",
    "print(\"\\n--- First 2 rows with .head(2) ---\")\n",
    "print(inventory_df.head(2))\n",
    "```\n",
    "\n",
    "**2. `.tail()`: Look at the last few rows**\n",
    "This is useful for checking if the data at the end of the file loaded correctly.\n",
    "\n",
    "```python\n",
    "# Show the last 5 rows\n",
    "print(\"\\n--- 2. Last 5 rows with .tail() ---\")\n",
    "print(inventory_df.tail())\n",
    "```\n",
    "\n",
    "**3. `.shape`: Check the dimensions**\n",
    "Just like with NumPy, this attribute tells you the number of rows and columns. It's crucial for understanding the size of your dataset.\n",
    "\n",
    "```python\n",
    "# Get the shape (rows, columns)\n",
    "print(f\"\\n--- 3. Shape of the DataFrame ---\")\n",
    "print(f\"The DataFrame has {inventory_df.shape[0]} rows and {inventory_df.shape[1]} columns.\")\n",
    "```\n",
    "\n",
    "**4. `.info()`: Get a technical summary**\n",
    "This is an incredibly useful method. It gives you a concise summary of the DataFrame, including:\n",
    "*   The number of rows and columns.\n",
    "*   The name and data type (`Dtype`) of each column.\n",
    "*   The number of **non-null** (non-missing) values in each column. (This is a first look at any potential missing data!).\n",
    "*   How much memory the DataFrame is using.\n",
    "\n",
    "```python\n",
    "print(\"\\n--- 4. Technical summary with .info() ---\")\n",
    "inventory_df.info()\n",
    "```\n",
    "\n",
    "**5. `.describe()`: Get a statistical summary**\n",
    "This method automatically calculates basic descriptive statistics for all the **numerical** columns.\n",
    "\n",
    "```python\n",
    "print(\"\\n--- 5. Statistical summary with .describe() ---\")\n",
    "print(inventory_df.describe())\n",
    "```\n",
    "This single command gives you the count, mean, standard deviation, min, max, and quartile values for the `price` and `quantity` columns.\n",
    "\n",
    "---\n",
    "\n",
    "These five methods—`.head()`, `.tail()`, `.shape`, `.info()`, and `.describe()`—are the \"first-look toolkit\" for any data analyst using Pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb2270-f97b-493f-ab78-0bcffedb72dc",
   "metadata": {},
   "source": [
    "### **3. Task**\n",
    "\n",
    "**Goal:** You will load a new, slightly larger dataset from a CSV file and then use the five inspection methods to perform an initial \"vital signs\" check on the data.\n",
    "\n",
    "#### **Step 1: Create the Data File**\n",
    "\n",
    "Create a new file named `sales_data.csv` and paste the following content into it. This represents sales records for a fictional store.\n",
    "\n",
    "```csv\n",
    "Date,Category,Product,Sales,Quantity\n",
    "2023-01-05,Electronics,Laptop,1200,1\n",
    "2023-01-05,Office,Pen,3,10\n",
    "2023-01-06,Electronics,Mouse,25,2\n",
    "2023-01-06,Home,Chair,150,1\n",
    "2023-01-07,Office,Notebook,5,5\n",
    "2023-01-08,Electronics,Keyboard,75,1\n",
    "2023-01-08,Home,Table,400,1\n",
    "2023-01-09,Electronics,Monitor,300,2\n",
    "2023-01-10,Office,Stapler,10,1\n",
    "2023-01-10,Home,Lamp,45,3\n",
    "```\n",
    "\n",
    "#### **Step 2: Write the Python Script**\n",
    "\n",
    "Create a new Python file named `data_inspector.py`.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Import Pandas** with its standard alias `pd`.\n",
    "2.  Use `pd.read_csv()` to load the data from `sales_data.csv` into a DataFrame named `sales_df`.\n",
    "3.  Use the five inspection methods we just discussed to learn about the `sales_df` DataFrame. For each step, print a clear, descriptive header so you know which output corresponds to which method.\n",
    "    *   Print the **first 5 rows**.\n",
    "    *   Print the **last 3 rows**.\n",
    "    *   Print the **shape** of the DataFrame.\n",
    "    *   Print the concise **technical summary** using `.info()`.\n",
    "    *   Print the **statistical summary** of the numerical columns.\n",
    "\n",
    "This task is straightforward and is designed to get you comfortable with the standard first steps of any data analysis project. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f72947-4efc-4ce3-aee7-320a883f064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five rows of the df:\n",
      "         Date     Category   Product  Sales  Quantity\n",
      "0  2023-01-05  Electronics    Laptop   1200         1\n",
      "1  2023-01-05       Office       Pen      3        10\n",
      "2  2023-01-06  Electronics     Mouse     25         2\n",
      "3  2023-01-06         Home     Chair    150         1\n",
      "4  2023-01-07       Office  Notebook      5         5\n",
      "\n",
      "Bottom three rows of the df:\n",
      "         Date     Category  Product  Sales  Quantity\n",
      "7  2023-01-09  Electronics  Monitor    300         2\n",
      "8  2023-01-10       Office  Stapler     10         1\n",
      "9  2023-01-10         Home     Lamp     45         3\n",
      "\n",
      "Shape of the df: (10, 5)\n",
      "\n",
      "Information about the df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Date      10 non-null     object\n",
      " 1   Category  10 non-null     object\n",
      " 2   Product   10 non-null     object\n",
      " 3   Sales     10 non-null     int64 \n",
      " 4   Quantity  10 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 532.0+ bytes\n",
      "\n",
      "Descriptive analysis of the df:              Sales   Quantity\n",
      "count    10.000000  10.000000\n",
      "mean    221.300000   2.700000\n",
      "std     369.860649   2.869379\n",
      "min       3.000000   1.000000\n",
      "25%      13.750000   1.000000\n",
      "50%      60.000000   1.500000\n",
      "75%     262.500000   2.750000\n",
      "max    1200.000000  10.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sales_df = pd.read_csv(\"sales_data.csv\")\n",
    "\n",
    "print(f\"Top five rows of the df:\\n{sales_df.head()}\\n\")\n",
    "print(f\"Bottom three rows of the df:\\n{sales_df.tail(3)}\\n\")\n",
    "print(f\"Shape of the df: {sales_df.shape}\\n\")\n",
    "print(\"Information about the df:\")\n",
    "sales_df.info()\n",
    "print(f\"\\nDescriptive analysis of the df: {sales_df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924453b9-665b-49ff-abbd-612317109fb1",
   "metadata": {},
   "source": [
    "### **Setting and Using an Index**\n",
    "\n",
    "As we saw, by default, a `DataFrame` has a numerical **index** starting from 0 (called a `RangeIndex`). This is useful, but often, one of your columns contains data that would serve as a much more meaningful row label.\n",
    "\n",
    "**Example:** For our `sales_data.csv`, the `Date` column is a perfect candidate for an index. It's often more intuitive to ask for \"sales on Jan 5th\" than \"sales in row 0\".\n",
    "\n",
    "The method to do this is **`.set_index()`**.\n",
    "\n",
    "#### **Example 1: Setting a New Index**\n",
    "\n",
    "Let's load our `sales_df` and set the `Date` column as the index.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "print(\"--- Original DataFrame with default index ---\")\n",
    "print(sales_df)\n",
    "\n",
    "# --- Setting the 'Date' column as the index ---\n",
    "# By default, .set_index() returns a NEW DataFrame. The original is unchanged.\n",
    "sales_df_with_date_index = sales_df.set_index('Date')\n",
    "\n",
    "print(\"\\n--- New DataFrame with 'Date' as the index ---\")\n",
    "print(sales_df_with_date_index)\n",
    "\n",
    "# The original DataFrame is still the same\n",
    "# print(\"\\n--- Original is unchanged ---\")\n",
    "# print(sales_df)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "--- Original DataFrame with default index ---\n",
    "         Date     Category   Product  Sales  Quantity\n",
    "0  2023-01-05  Electronics    Laptop   1200         1\n",
    "1  2023-01-05       Office       Pen      3        10\n",
    "...\n",
    "\n",
    "--- New DataFrame with 'Date' as the index ---\n",
    "                 Category   Product  Sales  Quantity\n",
    "Date                                               \n",
    "2023-01-05    Electronics    Laptop   1200         1\n",
    "2023-01-05         Office       Pen      3        10\n",
    "...\n",
    "```\n",
    "Notice how the `Date` column has moved to the far left, becoming the row labels.\n",
    "\n",
    "#### **The `inplace=True` Argument**\n",
    "\n",
    "Sometimes, you don't want to create a new DataFrame; you just want to modify the existing one. You can do this with the `inplace=True` argument.\n",
    "\n",
    "```python\n",
    "# Create a fresh copy to work with\n",
    "sales_df_copy = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# This time, we modify the DataFrame directly\n",
    "sales_df_copy.set_index('Date', inplace=True) \n",
    "\n",
    "# Now sales_df_copy itself is changed\n",
    "print(\"\\n--- DataFrame modified 'in place' ---\")\n",
    "print(sales_df_copy)\n",
    "```\n",
    "**Nuance:** Using `inplace=True` can be convenient, but many experienced Pandas users prefer the first method (creating a new DataFrame) because it makes the flow of data transformations clearer and helps prevent accidental changes to your original data. For our learning, both are fine to use.\n",
    "\n",
    "Now that we have a `DataFrame` with meaningful labels, we can properly learn `.loc` and `.iloc`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Selecting Data with `.loc` and `.iloc`**\n",
    "\n",
    "As we previewed:\n",
    "*   **`.loc`** = selects by **L**abel\n",
    "*   **`.iloc`** = selects by **i**nteger **loc**ation\n",
    "\n",
    "Let's use our `sales_df_with_date_index`.\n",
    "\n",
    "```python\n",
    "# Let's assume this df is our starting point\n",
    "df = sales_df.set_index('Date')\n",
    "\n",
    "# --- Using .loc (Label-based) ---\n",
    "# Get all sales that happened on the date '2023-01-08'\n",
    "# Notice the index can have duplicate labels!\n",
    "jan_8_sales = df.loc['2023-01-08']\n",
    "print(\"\\n--- All sales from 2023-01-08 (using .loc) ---\")\n",
    "print(jan_8_sales)\n",
    "\n",
    "\n",
    "# --- Using .iloc (Integer-location based) ---\n",
    "# Get the very first row (at position 0)\n",
    "first_row = df.iloc[0]\n",
    "print(\"\\n--- The first row of data (using .iloc[0]) ---\")\n",
    "print(first_row)\n",
    "\n",
    "\n",
    "# --- Combining Row and Column Selection ---\n",
    "# Get the 'Product' for the sale at date '2023-01-06'\n",
    "# Our index has two rows with this label. This will return both.\n",
    "product_on_jan_6 = df.loc['2023-01-06', 'Product']\n",
    "print(\"\\n--- Product sold on 2023-01-06 ---\")\n",
    "print(product_on_jan_6)\n",
    "\n",
    "# Get the value at row 3, column 2 (Sales value for the 4th record)\n",
    "value_at_3_2 = df.iloc[3, 2]\n",
    "print(f\"\\nValue at integer position [3, 2]: {value_at_3_2}\")\n",
    "```\n",
    "\n",
    "This is the standard and most powerful way to select subsets of your data in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70b141-77ca-4a6c-9db3-5e05ea9992ae",
   "metadata": {},
   "source": [
    "### **3. Task**\n",
    "\n",
    "**Goal:** Load the `sales_data.csv` file and perform a series of specific data selection operations to answer several questions.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Setup:**\n",
    "    *   Import Pandas.\n",
    "    *   Load the `sales_data.csv` file into a DataFrame named `sales_df`.\n",
    "\n",
    "2.  **Initial Selections (using the default 0-based index):**\n",
    "    *   Select and print only the `Product` and `Sales` columns.\n",
    "    *   Select and print the data for the row at **integer position 4** (the 5th row) using `.iloc`.\n",
    "    *   Select and print the **last two rows** of the DataFrame using `.iloc` and slicing.\n",
    "\n",
    "3.  **Index-based Selections:**\n",
    "    *   Set the `Date` column as the index for `sales_df`. Use `inplace=True` to modify the DataFrame directly.\n",
    "    *   Print a header and then print the newly indexed DataFrame to confirm the change.\n",
    "    *   Select and print all the sales data from the date **'2023-01-08'** using `.loc`.\n",
    "    *   Select and print the `Category` and `Quantity` for the sales that occurred from date **'2023-01-07' through '2023-01-10'** (inclusive). Use `.loc` with both row and column slicing.\n",
    "\n",
    "This task will walk you through the most common selection patterns you'll use in data analysis. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb70ec79-b2a0-4078-9bcd-b3981da6fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for Product and Sales:\n",
      "    Product  Sales\n",
      "0    Laptop   1200\n",
      "1       Pen      3\n",
      "2     Mouse     25\n",
      "3     Chair    150\n",
      "4  Notebook      5\n",
      "5  Keyboard     75\n",
      "6     Table    400\n",
      "7   Monitor    300\n",
      "8   Stapler     10\n",
      "9      Lamp     45\n",
      "\n",
      "Data for 5th row:\n",
      "Date        2023-01-07\n",
      "Category        Office\n",
      "Product       Notebook\n",
      "Sales                5\n",
      "Quantity             5\n",
      "Name: 4, dtype: object\n",
      "\n",
      "Last two rows of df:\n",
      "         Date Category  Product  Sales  Quantity\n",
      "8  2023-01-10   Office  Stapler     10         1\n",
      "9  2023-01-10     Home     Lamp     45         3\n",
      "\n",
      "Newly Indexed DataFrame:\n",
      "               Category   Product  Sales  Quantity\n",
      "Date                                              \n",
      "2023-01-05  Electronics    Laptop   1200         1\n",
      "2023-01-05       Office       Pen      3        10\n",
      "2023-01-06  Electronics     Mouse     25         2\n",
      "2023-01-06         Home     Chair    150         1\n",
      "2023-01-07       Office  Notebook      5         5\n",
      "2023-01-08  Electronics  Keyboard     75         1\n",
      "2023-01-08         Home     Table    400         1\n",
      "2023-01-09  Electronics   Monitor    300         2\n",
      "2023-01-10       Office   Stapler     10         1\n",
      "2023-01-10         Home      Lamp     45         3\n",
      "\n",
      "Sales data from 2023-01-08:\n",
      "               Category   Product  Sales  Quantity\n",
      "Date                                              \n",
      "2023-01-08  Electronics  Keyboard     75         1\n",
      "2023-01-08         Home     Table    400         1\n",
      "2023-01-09  Electronics   Monitor    300         2\n",
      "2023-01-10       Office   Stapler     10         1\n",
      "2023-01-10         Home      Lamp     45         3\n",
      "\n",
      "Category and Quantity for sales from 2023-01-07 to 2023-01-10:\n",
      "               Category  Quantity\n",
      "Date                             \n",
      "2023-01-07       Office         5\n",
      "2023-01-08  Electronics         1\n",
      "2023-01-08         Home         1\n",
      "2023-01-09  Electronics         2\n",
      "2023-01-10       Office         1\n",
      "2023-01-10         Home         3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sales_df = pd.read_csv(\"sales_data.csv\")\n",
    "print(f\"Data for Product and Sales:\\n{sales_df[[\"Product\", \"Sales\"]]}\\n\")\n",
    "print(f\"Data for 5th row:\\n{sales_df.iloc[4]}\\n\")\n",
    "print(f\"Last two rows of df:\\n{sales_df.iloc[-2:]}\\n\")\n",
    "\n",
    "#Setting the date column as index\n",
    "\n",
    "sales_df.set_index(\"Date\", inplace=True)\n",
    "print(\"Newly Indexed DataFrame:\")\n",
    "print(f\"{sales_df}\\n\")\n",
    "print(f\"Sales data from 2023-01-08:\\n{sales_df.loc[\"2023-01-08\":]}\\n\")\n",
    "print(f\"Category and Quantity for sales from 2023-01-07 to 2023-01-10:\\n{sales_df.loc[\"2023-01-07\":\"2023-01-10\", [\"Category\", \"Quantity\"]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd680a9-6ed6-4102-b354-030d4024e107",
   "metadata": {},
   "source": [
    "### **Conditional Filtering**\n",
    "\n",
    "This is the Pandas equivalent of NumPy's boolean indexing, and it's arguably the most important skill in data analysis. This is how you ask questions of your data.\n",
    "\n",
    "*   \"Which sales were over $100?\"\n",
    "*   \"Which products are in the 'Electronics' category?\"\n",
    "*   \"Which sales had a quantity of more than 1?\"\n",
    "\n",
    "The syntax is very intuitive and looks a lot like the NumPy version.\n",
    "\n",
    "**Example:** Find all sales over $100.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Step 1: Create the boolean Series (the \"mask\")\n",
    "# The condition is applied to the 'Sales' column\n",
    "is_over_100 = sales_df['Sales'] > 100\n",
    "print(\"--- The Boolean Mask ---\")\n",
    "print(is_over_100)\n",
    "\n",
    "# Step 2: Use the mask to filter the DataFrame\n",
    "# Pass the boolean Series inside the [] accessor\n",
    "large_sales_df = sales_df[is_over_100]\n",
    "print(\"\\n--- Sales over $100 ---\")\n",
    "print(large_sales_df)\n",
    "\n",
    "# The more common, one-line version:\n",
    "large_sales_df_oneline = sales_df[sales_df['Sales'] > 100]\n",
    "```\n",
    "\n",
    "This simple `df[df['column'] > value]` pattern is the foundation for almost all data filtering in Pandas.\n",
    "\n",
    "##### **Example 1: Filtering Based on Text (Equality)**\n",
    "\n",
    "Let's find all the sales that belong to the 'Electronics' category.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Find all rows where the 'Category' column is exactly 'Electronics'\n",
    "electronics_sales = sales_df[sales_df['Category'] == 'Electronics']\n",
    "\n",
    "print(\"--- All sales in the Electronics category ---\")\n",
    "print(electronics_sales)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- All sales in the Electronics category ---\n",
    "         Date     Category   Product  Sales  Quantity\n",
    "0  2023-01-05  Electronics    Laptop   1200         1\n",
    "2  2023-01-06  Electronics     Mouse     25         2\n",
    "5  2023-01-08  Electronics  Keyboard     75         1\n",
    "7  2023-01-09  Electronics   Monitor    300         2\n",
    "```\n",
    "\n",
    "##### **Example 2: Combining Conditions (`&` and `|`)**\n",
    "\n",
    "This is where it gets really powerful. You can combine multiple conditions to ask more complex questions.\n",
    "\n",
    "**Important:**\n",
    "*   Use `&` for **AND** (both conditions must be true).\n",
    "*   Use `|` for **OR** (at least one condition must be true).\n",
    "*   You **must** wrap each individual condition in parentheses `()` due to Python's operator precedence rules.\n",
    "\n",
    "Let's find all 'Electronics' sales that were also over $100.\n",
    "\n",
    "```python\n",
    "# Condition 1: Category is 'Electronics'\n",
    "cond1 = sales_df['Category'] == 'Electronics'\n",
    "\n",
    "# Condition 2: Sales are greater than 100\n",
    "cond2 = sales_df['Sales'] > 100\n",
    "\n",
    "# Combine the conditions with '&' (AND)\n",
    "big_electronics_sales = sales_df[cond1 & cond2]\n",
    "\n",
    "print(\"--- Electronics sales over $100 ---\")\n",
    "print(big_electronics_sales)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Electronics sales over $100 ---\n",
    "         Date     Category  Product  Sales  Quantity\n",
    "0  2023-01-05  Electronics   Laptop   1200         1\n",
    "7  2023-01-09  Electronics  Monitor    300         2\n",
    "```\n",
    "\n",
    "##### **Example 3: Filtering with `.isin()`**\n",
    "\n",
    "What if you want to find all sales from the 'Home' OR 'Office' categories? You could use `|`, but a cleaner way for checking against a list of possible values is the `.isin()` method.\n",
    "\n",
    "```python\n",
    "# Find all rows where the category is either 'Home' or 'Office'\n",
    "home_and_office_sales = sales_df[sales_df['Category'].isin(['Home', 'Office'])]\n",
    "\n",
    "print(\"\\n--- All sales from Home and Office categories ---\")\n",
    "print(home_and_office_sales)\n",
    "```\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   The fundamental pattern is `df[<boolean_series>]`.\n",
    "*   Use `==` for equality, `>` for greater than, etc.\n",
    "*   Combine conditions with `&` (AND) and `|` (OR), making sure to wrap each condition in `()`.\n",
    "*   Use `.isin()` for a clean way to check against a list of values.\n",
    "\n",
    "This skill—filtering a DataFrame based on one or more conditions—is something you will do in every single data analysis project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3188ab3-1c4c-4071-a67f-f485721724f8",
   "metadata": {},
   "source": [
    "# **3. Task**\n",
    "\n",
    "**Goal:** Load the `sales_data.csv` file and use conditional filtering to answer several specific business questions.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Setup:**\n",
    "    *   Import Pandas.\n",
    "    *   Load the `sales_data.csv` file into a DataFrame named `sales_df`.\n",
    "\n",
    "2.  **Perform the following filtering operations.** For each one, print a descriptive header and then print the resulting DataFrame.\n",
    "\n",
    "    *   **Question 1:** Find all sales records where the `Quantity` sold was exactly **1**.\n",
    "    *   **Question 2:** Find all sales records for the `Product` named **'Laptop'**.\n",
    "    *   **Question 3 (Combining Conditions):** Find all sales from the **'Office'** category that had `Sales` of **less than $10**.\n",
    "    *   **Question 4 (Using `.isin()`):** Find all sales records for products that are either a **'Laptop'** or a **'Monitor'**.\n",
    "\n",
    "This task will test your ability to apply single conditions, combine conditions, and use the `.isin()` method to extract specific, meaningful subsets of the data. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8fbb7b6-52e8-471e-b381-0c910f1ce44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sales records where sold quantity is 1:\n",
      "         Date     Category   Product  Sales  Quantity\n",
      "0  2023-01-05  Electronics    Laptop   1200         1\n",
      "3  2023-01-06         Home     Chair    150         1\n",
      "5  2023-01-08  Electronics  Keyboard     75         1\n",
      "6  2023-01-08         Home     Table    400         1\n",
      "8  2023-01-10       Office   Stapler     10         1\n",
      "\n",
      "All sales records for the product named 'Laptop':\n",
      "         Date     Category Product  Sales  Quantity\n",
      "0  2023-01-05  Electronics  Laptop   1200         1\n",
      "\n",
      "All sales records for the category named 'Office' that had sales less than $10:\n",
      "         Date Category   Product  Sales  Quantity\n",
      "1  2023-01-05   Office       Pen      3        10\n",
      "4  2023-01-07   Office  Notebook      5         5\n",
      "\n",
      "All sales records for the products 'Latptop' or 'Monitor':\n",
      "         Date     Category  Product  Sales  Quantity\n",
      "0  2023-01-05  Electronics   Laptop   1200         1\n",
      "7  2023-01-09  Electronics  Monitor    300         2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sales_df = pd.read_csv(\"sales_data.csv\")\n",
    "print(f\"All sales records where sold quantity is 1:\\n{sales_df[sales_df[\"Quantity\"]==1]}\\n\")\n",
    "print(f\"All sales records for the product named 'Laptop':\\n{sales_df[sales_df[\"Product\"]==\"Laptop\"]}\\n\")\n",
    "print(f\"All sales records for the category named 'Office' that had sales less than $10:\\n{sales_df[(sales_df[\"Category\"]==\"Office\") & (sales_df[\"Sales\"]<10)]}\\n\")\n",
    "print(f\"All sales records for the products 'Latptop' or 'Monitor':\\n{sales_df[sales_df[\"Product\"].isin([\"Laptop\",\"Monitor\"])]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90519cb3-74b8-49a8-8459-afe50cca3c87",
   "metadata": {},
   "source": [
    "### **Creating New Columns**\n",
    "\n",
    "Often, your analysis will require you to create new columns based on the data in existing columns. For example, our dataset has `Sales` and `Quantity`, but it doesn't have the `Price` per item. We can calculate that.\n",
    "\n",
    "Creating a new column in Pandas is incredibly simple and intuitive. You just assign a value to it as if it were a new key in a dictionary.\n",
    "\n",
    "**Example 1: Creating a column from existing columns**\n",
    "\n",
    "Let's calculate the `Price` for each item (`Sales / Quantity`).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Create a new column named 'Price'\n",
    "# The calculation is vectorized, just like in NumPy!\n",
    "sales_df['Price'] = sales_df['Sales'] / sales_df['Quantity']\n",
    "\n",
    "print(\"--- DataFrame with the new 'Price' column ---\")\n",
    "print(sales_df)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- DataFrame with the new 'Price' column ---\n",
    "         Date     Category   Product  Sales  Quantity    Price\n",
    "0  2023-01-05  Electronics    Laptop   1200         1   1200.0\n",
    "1  202as-01-05       Office       Pen      3        10      0.3\n",
    "...\n",
    "```\n",
    "It's that easy. Pandas performs the division element-wise, just like NumPy, and assigns the resulting `Series` to the new `Price` column.\n",
    "\n",
    "**Example 2: Creating a column with a constant value**\n",
    "\n",
    "Sometimes you want to add a new column with the same value for all rows.\n",
    "\n",
    "```python\n",
    "# Add a 'Tax_Rate' column with a constant value\n",
    "sales_df['Tax_Rate'] = 0.05\n",
    "\n",
    "print(\"\\n--- DataFrame with the 'Tax_Rate' column ---\")\n",
    "print(sales_df.head())\n",
    "```\n",
    "\n",
    "**Example 3: Creating a column based on a condition (Advanced Preview)**\n",
    "\n",
    "This is a bit more advanced, but it's a very common pattern. We can use NumPy's `np.where()` function to create a new column based on a condition.\n",
    "\n",
    "`np.where(condition, value_if_true, value_if_false)`\n",
    "\n",
    "Let's create a column `'Sale_Type'` that is 'Large' if the sale was > $100 and 'Small' otherwise.\n",
    "\n",
    "```python\n",
    "import numpy as np # We need numpy for this\n",
    "\n",
    "# Create the new column based on a condition\n",
    "sales_df['Sale_Type'] = np.where(sales_df['Sales'] > 100, 'Large', 'Small')\n",
    "\n",
    "print(\"\\n--- DataFrame with conditional 'Sale_Type' column ---\")\n",
    "print(sales_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff2752-41b9-4c13-a96b-233d8eec143c",
   "metadata": {},
   "source": [
    "### **3. Task**\n",
    "\n",
    "**Goal:** Load the `sales_data.csv` file, and then create three new columns based on the existing data to enrich the dataset.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Setup:**\n",
    "    *   Import both Pandas and NumPy with their standard aliases.\n",
    "    *   Load the `sales_data.csv` file into a DataFrame named `sales_df`.\n",
    "\n",
    "2.  **Create New Columns:**\n",
    "    *   **Task 1 (Calculation):** Create a new column named `'Price_Per_Item'`. This should be calculated by dividing the `'Sales'` column by the `'Quantity'` column.\n",
    "    *   **Task 2 (Constant Value):** Create a new column named `'Sales_Region'` and assign the constant string value `'North'` to all rows.\n",
    "    *   **Task 3 (Conditional Logic):** Create a new column named `'Is_High_Value'`. This column should contain the boolean value `True` if the `'Sales'` for that row is greater than $100, and `False` otherwise. *(Hint: You don't need `np.where` for this. A simple comparison `df['col'] > 100` will produce the boolean Series you need).*\n",
    "\n",
    "3.  **Verification:**\n",
    "    *   After creating all three columns, print the **first 5 rows** of the modified DataFrame using `.head()` to verify that your new columns have been added correctly.\n",
    "\n",
    "This task will test your ability to create new columns using all three methods we discussed: from a calculation, from a constant, and from a direct boolean condition. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb7684d-45f6-4e6f-91e8-bbace1d005da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     Category   Product  Sales  Quantity   Price Sales_Region  \\\n",
      "0  2023-01-05  Electronics    Laptop   1200         1  1200.0        North   \n",
      "1  2023-01-05       Office       Pen      3        10     0.3        North   \n",
      "2  2023-01-06  Electronics     Mouse     25         2    12.5        North   \n",
      "3  2023-01-06         Home     Chair    150         1   150.0        North   \n",
      "4  2023-01-07       Office  Notebook      5         5     1.0        North   \n",
      "\n",
      "   Is_High_Value  \n",
      "0           True  \n",
      "1          False  \n",
      "2          False  \n",
      "3           True  \n",
      "4          False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "sales_df['Price'] = sales_df['Sales'] / sales_df['Quantity']\n",
    "sales_df['Sales_Region'] = 'North'\n",
    "sales_df['Is_High_Value'] = sales_df['Sales'] > 100\n",
    "\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e64b5f-a9ae-44a1-8180-fd6cdb171ebf",
   "metadata": {},
   "source": [
    "### Another way to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "638a0d56-cfb5-4f10-9b21-ccfb94ce8ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     Category   Product  Sales  Quantity  Price_Per_Item  \\\n",
      "0  2023-01-05  Electronics    Laptop   1200         1          1200.0   \n",
      "1  2023-01-05       Office       Pen      3        10             0.3   \n",
      "2  2023-01-06  Electronics     Mouse     25         2            12.5   \n",
      "3  2023-01-06         Home     Chair    150         1           150.0   \n",
      "4  2023-01-07       Office  Notebook      5         5             1.0   \n",
      "\n",
      "  Sales_Region Is_High_Value  \n",
      "0        North          True  \n",
      "1        North         False  \n",
      "2        North         False  \n",
      "3        North          True  \n",
      "4        North         False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "sales_df['Price'] = sales_df['Sales'] / sales_df['Quantity']\n",
    "sales_df.rename(columns={'Price':'Price_Per_Item'}, inplace=True)\n",
    "sales_df['Sales_Region'] = 'North'\n",
    "sales_df['Is_High_Value'] = np.where(sales_df['Sales']>100, 'True', 'False')\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293af49-eccc-43ec-8f09-7ab25c9bf50e",
   "metadata": {},
   "source": [
    "### **Sorting and Aggregating Data**\n",
    "\n",
    "We've learned how to select data, filter it, and add to it. The next logical step is to organize and summarize it.\n",
    "\n",
    "#### **1. Sorting with `.sort_values()`**\n",
    "\n",
    "You can easily sort a DataFrame by the values in one or more columns.\n",
    "\n",
    "**Example 1: Sorting by a single column**\n",
    "Let's find our biggest sales by sorting the DataFrame by the `'Sales'` column.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Sort the DataFrame by the 'Sales' column\n",
    "# By default, it sorts in ascending order (smallest to largest)\n",
    "sorted_by_sales = sales_df.sort_values(by='Sales')\n",
    "\n",
    "print(\"--- Sorted by Sales (Ascending) ---\")\n",
    "print(sorted_by_sales)\n",
    "\n",
    "# To sort in descending order, use the 'ascending=False' argument\n",
    "top_sales = sales_df.sort_values(by='Sales', ascending=False)\n",
    "\n",
    "print(\"\\n--- Sorted by Sales (Descending) - Top Sales First ---\")\n",
    "print(top_sales.head()) # Use .head() to just see the top few\n",
    "```\n",
    "\n",
    "#### **2. Aggregating with `.groupby()`**\n",
    "\n",
    "This is one of the most powerful features in Pandas. It allows you to:\n",
    "1.  **Split** the data into groups based on some criteria.\n",
    "2.  **Apply** a function (like `sum`, `mean`, `count`) to each group independently.\n",
    "3.  **Combine** the results into a new DataFrame.\n",
    "\n",
    "This is called the **\"Split-Apply-Combine\"** pattern.\n",
    "\n",
    "**Example 2: Summarizing data by category**\n",
    "Let's find the total sales for each product category.\n",
    "\n",
    "```python\n",
    "# We want to group the DataFrame by the 'Category' column\n",
    "grouped_by_category = sales_df.groupby('Category')\n",
    "\n",
    "# Now that we have the groups, we can apply an aggregation function.\n",
    "# Let's get the sum of all numerical columns for each category.\n",
    "category_sales_sum = grouped_by_category.sum()\n",
    "\n",
    "print(\"\\n--- Total Sales and Quantity per Category ---\")\n",
    "print(category_sales_sum)\n",
    "\n",
    "# You can also select a specific column to aggregate\n",
    "# This is a very common and readable way to do it\n",
    "category_sales_mean = sales_df.groupby('Category')['Sales'].mean()\n",
    "\n",
    "print(\"\\n--- Average Sale Value per Category ---\")\n",
    "print(category_sales_mean)\n",
    "```\n",
    "\n",
    "**Output of the `.groupby()` examples:**\n",
    "```\n",
    "--- Total Sales and Quantity per Category ---\n",
    "             Sales  Quantity\n",
    "Category                      \n",
    "Electronics   1600         6\n",
    "Home           595         5\n",
    "Office          18        16\n",
    "\n",
    "--- Average Sale Value per Category ---\n",
    "Category\n",
    "Electronics    400.00\n",
    "Home           198.33\n",
    "Office           6.00\n",
    "Name: Sales, dtype: float64\n",
    "```\n",
    "\n",
    "The `.groupby()` method is the foundation of data summarization. It lets you move from looking at individual records to understanding the characteristics of entire groups within your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d4098-f5a8-4a7b-a7ec-8f4b8124b769",
   "metadata": {},
   "source": [
    "### **3. Task**\n",
    "\n",
    "**Goal:** Load the `sales_data.csv` file, and then use sorting and grouping to answer specific business questions about sales performance.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Setup:**\n",
    "    *   Import Pandas.\n",
    "    *   Load the `sales_data.csv` file into a DataFrame named `sales_df`.\n",
    "\n",
    "2.  **Sorting Analysis:**\n",
    "    *   Find the **5 least expensive sales** (the 5 rows with the lowest `Sales` values). Sort the DataFrame accordingly and print the top 5 rows of the result.\n",
    "    *   Find the **5 largest sales by quantity** (the 5 rows with the highest `Quantity` values). Sort the DataFrame and print the top 5 rows.\n",
    "\n",
    "3.  **Grouping and Aggregation Analysis:**\n",
    "    *   Calculate the **total number of items sold** (sum of `Quantity`) for each `Category`. Print the resulting Series.\n",
    "    *   Calculate the **average `Sales` value** for each `Category`. Print the resulting Series.\n",
    "\n",
    "This task will require you to use both `.sort_values()` (with both ascending and descending orders) and `.groupby()` to create insightful summaries of the data. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89852d7d-0ddd-4335-abfd-9d5644b1cb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five least expensive sales are:\n",
      "         Date     Category   Product  Sales  Quantity\n",
      "1  2023-01-05       Office       Pen      3        10\n",
      "4  2023-01-07       Office  Notebook      5         5\n",
      "8  2023-01-10       Office   Stapler     10         1\n",
      "2  2023-01-06  Electronics     Mouse     25         2\n",
      "9  2023-01-10         Home      Lamp     45         3\n",
      "\n",
      "The five largest sales are:\n",
      "         Date     Category   Product  Sales  Quantity\n",
      "1  2023-01-05       Office       Pen      3        10\n",
      "4  2023-01-07       Office  Notebook      5         5\n",
      "9  2023-01-10         Home      Lamp     45         3\n",
      "7  2023-01-09  Electronics   Monitor    300         2\n",
      "2  2023-01-06  Electronics     Mouse     25         2\n",
      "\n",
      "The total number of items sold for each Category are:\n",
      "Category\n",
      "Electronics     6\n",
      "Home            5\n",
      "Office         16\n",
      "Name: Quantity, dtype: int64\n",
      "\n",
      "Average sales value for each Category are:\n",
      "Category\n",
      "Electronics    400.000000\n",
      "Home           198.333333\n",
      "Office           6.000000\n",
      "Name: Sales, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sales_df = pd.read_csv('sales_data.csv')\n",
    "#sorting\n",
    "print(f\"The five least expensive sales are:\\n{sales_df.sort_values(by='Sales').head()}\\n\")\n",
    "print(f\"The five largest sales are:\\n{sales_df.sort_values(by='Quantity', ascending=False).head()}\\n\")\n",
    "\n",
    "#grouping\n",
    "print(f\"The total number of items sold for each Category are:\\n{sales_df.groupby('Category')['Quantity'].sum()}\\n\")\n",
    "print(f\"Average sales value for each Category are:\\n{sales_df.groupby('Category')['Sales'].mean()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd6145-d01b-4f75-a5ab-f8e8a17a1978",
   "metadata": {},
   "source": [
    "### **Pandas Proficiency Challenge: Employee Data Analysis**\n",
    "\n",
    "#### **Step 1: The Dataset**\n",
    "\n",
    "Create a new file named `employee_data.csv` and paste the following data into it.\n",
    "\n",
    "```csv\n",
    "EmployeeID,FirstName,LastName,Department,StartDate,Salary\n",
    "E1021,Alice,Smith,HR,2022-03-15,60000\n",
    "E1022,Bob,Johnson,Engineering,2021-11-01,95000\n",
    "E1023,Charlie,Williams,Sales,2022-08-20,78000\n",
    "E1024,David,Brown,Engineering,2023-01-10,110000\n",
    "E1025,Eve,Jones,Sales,2021-05-30,85000\n",
    "E1026,Frank,Garcia,HR,2023-02-28,62000\n",
    "E1027,Grace,Miller,Engineering,2022-07-12,98000\n",
    "E1028,Henry,Davis,Marketing,2023-01-15,70000\n",
    "E1029,Ivy,Rodriguez,Sales,2022-12-01,72000\n",
    "E1030,Jack,Martinez,Engineering,2021-09-25,125000\n",
    "```\n",
    "\n",
    "#### **Step 2: The Analysis Script**\n",
    "\n",
    "Create a Python file named `employee_analyzer.py`.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "Your script should load the `employee_data.csv` file and answer the following questions by printing the results to the console.\n",
    "\n",
    "1.  **Initial Inspection:**\n",
    "    *   Load the data into a DataFrame.\n",
    "    *   Set the `EmployeeID` column as the index.\n",
    "    *   Display the `.info()` summary of the DataFrame.\n",
    "\n",
    "2.  **Highest Paid Engineer:**\n",
    "    *   Find the full record (the entire row) for the employee in the 'Engineering' department with the highest salary.\n",
    "\n",
    "3.  **New Employee Bonus:**\n",
    "    *   The company is giving a $2,000 bonus to all employees who started on or after January 1st, 2023.\n",
    "    *   Create a new column called `'Bonus'` in the DataFrame. This column should contain `2000` for eligible employees and `0` for everyone else.\n",
    "    *   Display the `FirstName`, `LastName`, `StartDate`, and `Bonus` columns for all employees.\n",
    "\n",
    "4.  **Departmental Salary Report:**\n",
    "    *   Calculate the **average salary** for each department.\n",
    "    *   Display the results sorted from the highest average salary to the lowest.\n",
    "\n",
    "5.  **Sales Team Analysis:**\n",
    "    *   Create a new DataFrame that contains **only** the employees from the 'Sales' department.\n",
    "    *   From this new DataFrame, find the record for the sales employee with the **lowest salary**.\n",
    "\n",
    "Good luck. This will require you to chain several operations together to get the answers. I am ready for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48325792-2922-4173-9656-eaa4688fa93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial information about the dataframe:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10 entries, E1021 to E1030\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   FirstName   10 non-null     object\n",
      " 1   LastName    10 non-null     object\n",
      " 2   Department  10 non-null     object\n",
      " 3   StartDate   10 non-null     object\n",
      " 4   Salary      10 non-null     int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 480.0+ bytes\n",
      "\n",
      "The employee in Engineering department with the highest salary:\n",
      "           FirstName  LastName   Department   StartDate  Salary\n",
      "EmployeeID                                                     \n",
      "E1030           Jack  Martinez  Engineering  2021-09-25  125000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#loading the data from the csv file\n",
    "employee_df = pd.read_csv('employee_data.csv')\n",
    "\n",
    "#setting EmployeeID as Index\n",
    "employee_df.set_index('EmployeeID', inplace=True)\n",
    "\n",
    "#displaying the information of the dataframe\n",
    "print(\"Initial information about the dataframe:\")\n",
    "employee_df.info()\n",
    "\n",
    "engineering_department = employee_df['Department'] == 'Engineering'\n",
    "print(f\"\\nThe employee in Engineering department with the highest salary:\\n{employee_df[engineering_department].sort_values('Salary', ascending=False).head(1)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbfb14-a9a5-4c4c-a54b-5365516201d3",
   "metadata": {},
   "source": [
    "### **Micro-Lesson: Working with Dates and Times in Pandas**\n",
    "\n",
    "**The Problem:**\n",
    "As you discovered, when you load a CSV, columns containing dates are usually read in as generic `object` types (strings).\n",
    "\n",
    "```python\n",
    "# df['StartDate'].dtype -> object\n",
    "```\n",
    "You can't perform date-based comparisons on strings (e.g., you can't easily find all dates *after* a certain day, or calculate the *duration* between two dates).\n",
    "\n",
    "**The Solution: The `datetime` Data Type**\n",
    "Pandas has a special data type called `datetime64[ns]` which is specifically designed to handle dates and times. It understands the calendar, the order of dates, and allows for powerful time-based operations.\n",
    "\n",
    "The key function to get there is **`pd.to_datetime()`**.\n",
    "\n",
    "**How it Works:**\n",
    "This function is incredibly smart. You give it a Series of date-like strings, and it does its best to parse them and convert them into a Series of proper datetime objects.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('employee_data.csv')\n",
    "print(\"--- Before Conversion ---\")\n",
    "df.info() # Notice StartDate is 'object'\n",
    "\n",
    "# --- The Conversion Step ---\n",
    "# Overwrite the old string column with the new datetime column\n",
    "df['StartDate'] = pd.to_datetime(df['StartDate'])\n",
    "\n",
    "print(\"\\n--- After Conversion ---\")\n",
    "df.info() # Notice StartDate is now 'datetime64[ns]'\n",
    "```\n",
    "**Output Change:**\n",
    "```\n",
    "--- Before Conversion ---\n",
    "...\n",
    " 4   StartDate  10 non-null     object  <-- It's a string\n",
    "...\n",
    "--- After Conversion ---\n",
    "...\n",
    " 4   StartDate  10 non-null     datetime64[ns] <-- Now it's a datetime!\n",
    "...\n",
    "```\n",
    "\n",
    "**What can you do now?**\n",
    "Once the column is in the `datetime` format, you can perform logical comparisons directly, just like you hoped. Pandas is smart enough to understand that comparing a datetime Series to a date string works.\n",
    "\n",
    "```python\n",
    "# Now this comparison works perfectly!\n",
    "# Pandas will parse '2023-01-01' on the fly for the comparison.\n",
    "is_new_employee = df['StartDate'] >= '2023-01-01'\n",
    "\n",
    "print(\"\\n--- Boolean mask for new employees ---\")\n",
    "print(is_new_employee)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Boolean mask for new employees ---\n",
    "0    False\n",
    "1    False\n",
    "2    False\n",
    "3     True\n",
    "4    False\n",
    "5     True\n",
    "6    False\n",
    "7     True\n",
    "8    False\n",
    "9    False\n",
    "Name: StartDate, dtype: bool\n",
    "```\n",
    "This boolean mask is exactly what you need to solve Requirement #3 of the challenge. You can now use this mask with `np.where()` or other methods to create your `'Bonus'` column.\n",
    "\n",
    "### `pd.to_datetime()` is incredibly smart\n",
    "\n",
    "The `pd.to_datetime()` function is designed to be very flexible and intelligent. It can automatically recognize and parse a wide variety of common date formats without you having to tell it what the format is.\n",
    "\n",
    "**It can handle all of these automatically:**\n",
    "*   `'2023-01-15'` (YYYY-MM-DD) - The standard ISO format.\n",
    "*   `'01/15/2023'` (MM/DD/YYYY) - Common US format.\n",
    "*   `'15/01/2023'` (DD/MM/YYYY) - Common European format (sometimes needs a hint).\n",
    "*   `'Jan 15, 2023'`\n",
    "*   `'2023-Jan-15'`\n",
    "*   `'20230115'` (YYYYMMDD)\n",
    "\n",
    "In most cases, you can just pass your column to the function and it will figure it out.\n",
    "\n",
    "### When it gets tricky (and how to solve it)\n",
    "\n",
    "**1. Ambiguous Formats (e.g., `'03-04-2023'`)**\n",
    "Does `'03-04-2023'` mean March 4th or April 3rd? This is a common problem.\n",
    "*   By default, Pandas often assumes the American-style `MM-DD-YYYY` format.\n",
    "*   **The Fix:** You can give Pandas a hint by using the `dayfirst=True` argument.\n",
    "    ```python\n",
    "    pd.to_datetime('03-04-2023', dayfirst=True) # -> Will correctly parse as April 3rd\n",
    "    ```\n",
    "\n",
    "**2. Non-Standard or Complex Formats (e.g., `'Wednesday, 15 of Jan, 23'`)**\n",
    "Sometimes, you have a really unusual or inconsistent date format.\n",
    "*   **The Fix:** For these rare cases, you can provide an explicit `format` code to tell Pandas exactly how to read the string. This works just like the `strftime` codes we saw in the `datetime` module.\n",
    "    ```python\n",
    "    # %A = Full weekday name, %d = Day of month, %b = Abbreviated month, %y = 2-digit year\n",
    "    pd.to_datetime('Wednesday, 15 of Jan, 23', format='%A, %d of %b, %y')\n",
    "    ```\n",
    "You will rarely need to do this, but it's an incredibly powerful tool to have when you encounter truly messy data.\n",
    "\n",
    "**Conclusion for your current task:**\n",
    "\n",
    "The format in our `employee_data.csv` (`'YYYY-MM-DD'`) is the international standard (ISO 8601). It is the most unambiguous and reliable format. `pd.to_datetime()` will handle it perfectly and without any extra arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "622cfee1-ceff-49db-963c-c2927b6b2254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial information about the dataframe:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10 entries, E1021 to E1030\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   FirstName   10 non-null     object\n",
      " 1   LastName    10 non-null     object\n",
      " 2   Department  10 non-null     object\n",
      " 3   StartDate   10 non-null     object\n",
      " 4   Salary      10 non-null     int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 480.0+ bytes\n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "The employee in Engineering department with the highest salary:\n",
      "\n",
      "           FirstName  LastName   Department   StartDate  Salary\n",
      "EmployeeID                                                     \n",
      "E1030           Jack  Martinez  Engineering  2021-09-25  125000\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "DataFrame after addition of the new Bonus feature:\n",
      "\n",
      "           FirstName   LastName  StartDate  Bonus\n",
      "EmployeeID                                       \n",
      "E1021          Alice      Smith 2022-03-15      0\n",
      "E1022            Bob    Johnson 2021-11-01      0\n",
      "E1023        Charlie   Williams 2022-08-20      0\n",
      "E1024          David      Brown 2023-01-10   2000\n",
      "E1025            Eve      Jones 2021-05-30      0\n",
      "E1026          Frank     Garcia 2023-02-28   2000\n",
      "E1027          Grace     Miller 2022-07-12      0\n",
      "E1028          Henry      Davis 2023-01-15   2000\n",
      "E1029            Ivy  Rodriguez 2022-12-01      0\n",
      "E1030           Jack   Martinez 2021-09-25      0\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "Department wise report on Average Salaries (Highest to Lowest)\n",
      "\n",
      "Department\n",
      "Engineering    107000.000000\n",
      "Sales           78333.333333\n",
      "Marketing       70000.000000\n",
      "HR              61000.000000\n",
      "Name: Salary, dtype: float64\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "Lowest salary employee from sales department:\n",
      "\n",
      "           FirstName   LastName Department  StartDate  Salary  Bonus\n",
      "EmployeeID                                                          \n",
      "E1029            Ivy  Rodriguez      Sales 2022-12-01   72000      0\n",
      "\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#loading the data from the csv file\n",
    "employee_df = pd.read_csv('employee_data.csv')\n",
    "\n",
    "#setting EmployeeID as Index\n",
    "employee_df.set_index('EmployeeID', inplace=True)\n",
    "\n",
    "#displaying the information of the dataframe\n",
    "print(\"Initial information about the dataframe:\\n\")\n",
    "employee_df.info()\n",
    "print(\"\\n------\\n\")\n",
    "\n",
    "#highest paid engineer\n",
    "engineering_department = employee_df['Department'] == 'Engineering'\n",
    "print(f\"\\nThe employee in Engineering department with the highest salary:\\n\\n{employee_df[engineering_department].sort_values('Salary', ascending=False).head(1)}\\n\")\n",
    "print(\"\\n------\\n\")\n",
    "\n",
    "#new employee bonus column\n",
    "employee_df['StartDate'] = pd.to_datetime(employee_df['StartDate'])\n",
    "employee_df['Bonus'] = np.where(employee_df['StartDate']>='2023-01-01', 2000, 0)\n",
    "print(f\"DataFrame after addition of the new Bonus feature:\\n\\n{employee_df[['FirstName', 'LastName', 'StartDate', 'Bonus']]}\\n\")\n",
    "print(\"\\n------\\n\")\n",
    "\n",
    "#departmental salary report\n",
    "groupby_department = employee_df.groupby('Department')\n",
    "print(f\"Department wise report on Average Salaries (Highest to Lowest)\\n\\n{groupby_department['Salary'].mean().sort_values(ascending=False)}\\n\")\n",
    "print(\"\\n------\\n\")\n",
    "\n",
    "#lowest salary employee from sales\n",
    "sales_employee_df = employee_df[employee_df['Department'] == 'Sales']\n",
    "print(f\"Lowest salary employee from sales department:\\n\\n{sales_employee_df.sort_values(by='Salary').head(1)}\")\n",
    "print(\"\\n------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "088357d5-61c7-4dd8-993f-b32b2352f833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name        5\n",
       "category    5\n",
       "value1      5\n",
       "value2      5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'name': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'category': ['X', 'Y', 'X', 'Y', 'Z'],\n",
    "    'value1': [10, 20, 30, 40, 50],\n",
    "    'value2': [5, 15, 25, 35, 45]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5ef91-95f6-4d30-a0f2-ab096cb87917",
   "metadata": {},
   "source": [
    "### **Combining DataFrames**\n",
    "\n",
    "#### **1. Connecting the Dots**\n",
    "\n",
    "**What it is:** Combining DataFrames is the process of taking two or more separate tables of data and merging them into a single, more informative table based on a shared column or index.\n",
    "\n",
    "**Why it's essential:** This is how you create a complete picture from fragmented information.\n",
    "*   You might have one file with **customer information** (ID, Name, City).\n",
    "*   You might have a separate file with all **sales transactions** (Transaction ID, Customer ID, Product, Amount).\n",
    "\n",
    "Neither file on its own can answer the question, \"Which city generates the most sales?\" To answer that, you **must** combine them, linking the sales transactions to the customer information using the shared `Customer ID`.\n",
    "\n",
    "**Analogy: Assembling a Profile**\n",
    "Imagine you have two lists:\n",
    "1.  A list of students and their `student_id`.\n",
    "2.  A separate list of `student_id`s and the `course` they are enrolled in.\n",
    "\n",
    "To get a full class roster showing each student's name and their course, you would match the rows from both lists where the `student_id` is the same. This \"matching\" or \"linking\" is the core idea of merging.\n",
    "\n",
    "**The Main Tool: `pd.merge()`**\n",
    "The primary function for this in Pandas is `pd.merge()`. It's a powerful and flexible function that performs database-style joins.\n",
    "\n",
    "The basic syntax is:\n",
    "`pd.merge(left_dataframe, right_dataframe, on='common_column_name')`\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Provide Simple Examples**\n",
    "\n",
    "Let's create two simple DataFrames to see how this works.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# The 'left' DataFrame with student information\n",
    "students_df = pd.DataFrame({\n",
    "    'student_id': ['S1', 'S2', 'S3', 'S4'],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "})\n",
    "\n",
    "# The 'right' DataFrame with course enrollments\n",
    "# Note: Student S4 (David) is not in this list, and course C103 has an unknown student (S5)\n",
    "courses_df = pd.DataFrame({\n",
    "    'student_id': ['S1', 'S2', 'S3', 'S5'],\n",
    "    'course': ['Math', 'History', 'Math', 'Physics']\n",
    "})\n",
    "\n",
    "print(\"--- Students DataFrame ---\")\n",
    "print(students_df)\n",
    "print(\"\\n--- Courses DataFrame ---\")\n",
    "print(courses_df)\n",
    "```\n",
    "\n",
    "Now, let's merge them.\n",
    "\n",
    "##### **Example 1: The Inner Join (The Default)**\n",
    "\n",
    "An \"inner\" join keeps **only** the rows where the key (`student_id` in this case) exists in **both** DataFrames.\n",
    "\n",
    "```python\n",
    "# Merge the two DataFrames on the 'student_id' column\n",
    "# By default, how='inner'\n",
    "merged_inner_df = pd.merge(students_df, courses_df, on='student_id')\n",
    "\n",
    "print(\"\\n--- Merged with an Inner Join ---\")\n",
    "print(merged_inner_df)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Merged with an Inner Join ---\n",
    "  student_id     name   course\n",
    "0         S1    Alice     Math\n",
    "1         S2      Bob  History\n",
    "2         S3  Charlie     Math\n",
    "```\n",
    "*Notice:* `David (S4)` is gone because he wasn't in the `courses_df`. The `Physics` course `(S5)` is gone because that student wasn't in the `students_df`.\n",
    "\n",
    "##### **Example 2: The Left Join**\n",
    "\n",
    "A \"left\" join keeps **all** the rows from the **left** DataFrame (`students_df`) and merges in data from the right where it finds a match. If there's no match, it fills with `NaN` (Not a Number).\n",
    "\n",
    "```python\n",
    "# Perform a left join\n",
    "merged_left_df = pd.merge(students_df, courses_df, on='student_id', how='left')\n",
    "\n",
    "print(\"\\n--- Merged with a Left Join ---\")\n",
    "print(merged_left_df)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Merged with a Left Join ---\n",
    "  student_id     name   course\n",
    "0         S1    Alice     Math\n",
    "1         S2      Bob  History\n",
    "2         S3  Charlie     Math\n",
    "3         S4    David      NaN  <-- David is kept, but his course is NaN (missing)\n",
    "```\n",
    "This is useful when you want to keep your master list of students and see which ones might be missing enrollment information.\n",
    "\n",
    "##### **3. The `outer` Join**\n",
    "\n",
    "We've seen `inner` (only matching keys), `left` (all from left), and `right` (all from right). The final main type of join is the **`outer` join**.\n",
    "\n",
    "**What it is:** An `outer` join keeps **all** rows from **both** DataFrames.\n",
    "*   If a row has a matching key, the data is combined.\n",
    "*   If a row from the left DataFrame has no match in the right, its corresponding right-side columns are filled with `NaN`.\n",
    "*   If a row from the right DataFrame has no match in the left, its corresponding left-side columns are filled with `NaN`.\n",
    "\n",
    "**Analogy:** An outer join is like saying, \"Give me a master list of everyone and everything, and match them up where you can. If you can't find a match for someone or something, still include them in the list, just with blank information for the part that's missing.\"\n",
    "\n",
    "Let's use our `students_df` and `courses_df` from before.\n",
    "*   `students_df` has `S1, S2, S3, S4` (David).\n",
    "*   `courses_df` has `S1, S2, S3, S5` (Physics).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "students_df = pd.DataFrame({\n",
    "    'student_id': ['S1', 'S2', 'S3', 'S4'],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "})\n",
    "courses_df = pd.DataFrame({\n",
    "    'student_id': ['S1', 'S2', 'S3', 'S5'],\n",
    "    'course': ['Math', 'History', 'Math', 'Physics']\n",
    "})\n",
    "\n",
    "# Perform an outer join\n",
    "merged_outer_df = pd.merge(students_df, courses_df, on='student_id', how='outer')\n",
    "\n",
    "print(\"\\n--- Merged with an Outer Join ---\")\n",
    "print(merged_outer_df)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Merged with an Outer Join ---\n",
    "  student_id     name   course\n",
    "0         S1    Alice     Math\n",
    "1         S2      Bob  History\n",
    "2         S3  Charlie     Math\n",
    "3         S4    David      NaN  <-- Kept from left, no match on right\n",
    "4         S5      NaN  Physics  <-- Kept from right, no match on left\n",
    "```\n",
    "As you can see, both `David (S4)` and the `Physics course (S5)` are included in the final result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cfb97-abb1-4fe0-80b7-04ea436d1d3b",
   "metadata": {},
   "source": [
    "### **3. Task**\n",
    "\n",
    "**Goal:** You will create two separate DataFrames—one for employee details and one for their department information—and then merge them to create a single, complete employee report. This is a very common real-world scenario.\n",
    "\n",
    "#### **Step 1: Create the Data**\n",
    "\n",
    "You will create two DataFrames from scratch.\n",
    "\n",
    "**DataFrame 1: `employees_df`**\n",
    "Create this DataFrame from a dictionary. It should contain employee IDs, their names, and the ID of the department they work in.\n",
    "\n",
    "| employee_id | name | dept_id |\n",
    "| :--- | :--- | :--- |\n",
    "| E1 | Alice | D1 |\n",
    "| E2 | Bob | D2 |\n",
    "| E3 | Charlie | D1 |\n",
    "| E4 | David | D3 |\n",
    "\n",
    "**DataFrame 2: `departments_df`**\n",
    "Create this DataFrame. It's a lookup table that maps department IDs to department names. Notice that there's a department (`D4`) that has no employees yet.\n",
    "\n",
    "| dept_id | dept_name |\n",
    "| :--- | :--- |\n",
    "| D1 | Sales |\n",
    "| D2 | Engineering |\n",
    "| D3 | Marketing |\n",
    "| D4 | HR |\n",
    "\n",
    "#### **Step 2: Perform the Merges**\n",
    "\n",
    "1.  **Import Pandas.**\n",
    "2.  Create the two DataFrames as described above.\n",
    "3.  **Perform an Inner Join:**\n",
    "    *   Merge `employees_df` and `departments_df` using an **inner** join. The common column is `'dept_id'`.\n",
    "    *   Store the result in a new DataFrame called `employee_report_inner`.\n",
    "    *   Print a header and then print `employee_report_inner`.\n",
    "4.  **Perform a Left Join:**\n",
    "    *   Merge `employees_df` and `departments_df` again, but this time use a **left** join. Make sure `employees_df` is the \"left\" DataFrame.\n",
    "    *   Store the result in `employee_report_left`.\n",
    "    *   Print a header and then print `employee_report_left`.\n",
    "5.  **Perform a Right Join:**\n",
    "    *   Merge the two DataFrames one last time using a **right** join.\n",
    "    *   Store the result in `employee_report_right`.\n",
    "    *   Print a header and then print `employee_report_right`.\n",
    "\n",
    "By doing all three joins, you will clearly see how the `how` parameter changes the output and which records are kept or discarded in each case. Good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fb90e2-662f-4382-a5fc-5af03a2cb3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees and their corresponding departments:\n",
      "\n",
      "  employee_id     name dept_id    dept_name\n",
      "0          E1    Alice      D1        Sales\n",
      "1          E2      Bob      D2  Engineering\n",
      "2          E3  Charlie      D1        Sales\n",
      "3          E4    David      D3    Marketing\n",
      "\n",
      "------\n",
      "Left join on employee and departments dataframe:\n",
      "\n",
      "  employee_id     name dept_id    dept_name\n",
      "0          E1    Alice      D1        Sales\n",
      "1          E2      Bob      D2  Engineering\n",
      "2          E3  Charlie      D1        Sales\n",
      "3          E4    David      D3    Marketing\n",
      "\n",
      "------\n",
      "Right join on employee and departments dataframe:\n",
      "\n",
      "  employee_id     name dept_id    dept_name\n",
      "0          E1    Alice      D1        Sales\n",
      "1          E3  Charlie      D1        Sales\n",
      "2          E2      Bob      D2  Engineering\n",
      "3          E4    David      D3    Marketing\n",
      "4         NaN      NaN      D4           HR\n",
      "\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "employees_dict = {\n",
    "    'employee_id': ['E1', 'E2', 'E3', 'E4'],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'dept_id': ['D1', 'D2', 'D1', 'D3']\n",
    "}\n",
    "\n",
    "departments_dict = {\n",
    "    'dept_id': ['D1', 'D2', 'D3', 'D4'],\n",
    "    'dept_name': ['Sales', 'Engineering', 'Marketing', 'HR']\n",
    "}\n",
    "\n",
    "employees_df = pd.DataFrame(employees_dict)\n",
    "departments_df = pd.DataFrame(departments_dict)\n",
    "\n",
    "employee_report_inner = pd.merge(employees_df, departments_df, on='dept_id', how='inner')\n",
    "print(f\"Employees and their corresponding departments:\\n\\n{employee_report_inner}\\n\")\n",
    "print(\"------\")\n",
    "\n",
    "employee_report_left = pd.merge(employees_df, departments_df, on='dept_id', how='left')\n",
    "print(f\"Left join on employee and departments dataframe:\\n\\n{employee_report_left}\\n\")\n",
    "print(\"------\")\n",
    "\n",
    "employee_report_right = pd.merge(employees_df, departments_df, on='dept_id', how='right')\n",
    "print(f\"Right join on employee and departments dataframe:\\n\\n{employee_report_right}\\n\")\n",
    "print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca930b-409b-47e0-aaee-d672387645b7",
   "metadata": {},
   "source": [
    "### **The Final Pandas Concept: Concatenation**\n",
    "\n",
    "Merging is for combining DataFrames based on a shared key, like joining columns side-by-side.\n",
    "\n",
    "**Concatenation**, on the other hand, is for **stacking DataFrames on top of each other** (or side-by-side). The primary function for this is `pd.concat()`.\n",
    "\n",
    "**Analogy:**\n",
    "*   **Merge:** Stitching two separate documents together based on a common reference number.\n",
    "*   **Concatenate:** Taking two separate lists and just sticking one to the end of the other to make a single, longer list.\n",
    "\n",
    "**Example:** Imagine you have sales data from two different months in two separate files. You just want to combine them into one big table.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Data for January\n",
    "jan_sales = pd.DataFrame({\n",
    "    'Product': ['A', 'B'],\n",
    "    'Sales': [100, 150]\n",
    "})\n",
    "\n",
    "# Data for February\n",
    "feb_sales = pd.DataFrame({\n",
    "    'Product': ['A', 'C'],\n",
    "    'Sales': [120, 50]\n",
    "})\n",
    "\n",
    "# Concatenate them by stacking them vertically (axis=0 is the default)\n",
    "all_sales = pd.concat([jan_sales, feb_sales])\n",
    "\n",
    "print(\"--- Vertically Concatenated DataFrame ---\")\n",
    "print(all_sales)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Vertically Concatenated DataFrame ---\n",
    "  Product  Sales\n",
    "0       A    100\n",
    "1       B    150\n",
    "0       A    120\n",
    "1       C     50\n",
    "```\n",
    "**The Problem:** Look at the index! It's `0, 1, 0, 1`. This is messy and can cause problems later.\n",
    "\n",
    "**The Fix:** You can tell `concat` to ignore the original indices and create a new, clean one.\n",
    "```python\n",
    "# Use the ignore_index=True argument\n",
    "all_sales_clean_index = pd.concat([jan_sales, feb_sales], ignore_index=True)\n",
    "\n",
    "print(\"\\n--- Concatenated with a Clean Index ---\")\n",
    "print(all_sales_clean_index)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Concatenated with a Clean Index ---\n",
    "  Product  Sales\n",
    "0       A    100\n",
    "1       B    150\n",
    "2       A    120\n",
    "3       C     50\n",
    "```\n",
    "This is much better. `pd.concat()` is the tool you use when you have multiple files with the exact same column structure that you just want to stack into a single, larger dataset.\n",
    "\n",
    "#### **2. Concatenating with `axis=1` (Side-by-Side)**\n",
    "\n",
    "We've used `pd.concat` to stack DataFrames vertically. By changing the `axis` parameter, you can also use it to glue them together horizontally.\n",
    "\n",
    "**`pd.concat(..., axis=1)`**\n",
    "\n",
    "This is useful when you have two DataFrames with the **same index** and you want to combine their columns.\n",
    "\n",
    "**Example:**\n",
    "Imagine you have one DataFrame with student names and another with their test scores, both indexed by `student_id`.\n",
    "\n",
    "```python\n",
    "# Student names, indexed by ID\n",
    "student_names = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie']\n",
    "}, index=['S1', 'S2', 'S3'])\n",
    "\n",
    "# Student scores, indexed by ID. Note: a different order and a missing student (S1)\n",
    "student_scores = pd.DataFrame({\n",
    "    'score': [88, 95]\n",
    "}, index=['S3', 'S2'])\n",
    "\n",
    "\n",
    "print(\"--- Names DataFrame ---\")\n",
    "print(student_names)\n",
    "print(\"\\n--- Scores DataFrame ---\")\n",
    "print(student_scores)\n",
    "\n",
    "# Concatenate horizontally. Pandas will align the data based on the index.\n",
    "combined_df = pd.concat([student_names, student_scores], axis=1)\n",
    "\n",
    "print(\"\\n--- Concatenated with axis=1 ---\")\n",
    "print(combined_df)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "--- Names DataFrame ---\n",
    "      name\n",
    "S1   Alice\n",
    "S2     Bob\n",
    "S3 Charlie\n",
    "\n",
    "--- Scores DataFrame ---\n",
    "    score\n",
    "S3     88\n",
    "S2     95\n",
    "\n",
    "--- Concatenated with axis=1 ---\n",
    "      name  score\n",
    "S1   Alice    NaN  <-- Alice has a name but no score\n",
    "S2     Bob   95.0\n",
    "S3 Charlie   88.0\n",
    "```\n",
    "Pandas automatically aligns the data on the index. Where an index label exists in one DataFrame but not the other (like `S1`), the corresponding value is filled with `NaN`.\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   `pd.merge()` is the function to combine DataFrames based on a common column (a \"key\").\n",
    "*   The `on` parameter specifies the key.\n",
    "*   The `how` parameter (`'inner'`, `'left'`, `'right'`, `'outer'`) controls which rows are kept in the final result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16d2fd-5e12-4c7a-a4cb-a2916ab436c1",
   "metadata": {},
   "source": [
    "## **3. Task**\n",
    "\n",
    "**Goal:** You have been given sales data from three different store locations, each in its own DataFrame. Your task is to combine them into a single master sales report.\n",
    "\n",
    "**The Data:**\n",
    "\n",
    "Create the following three DataFrames.\n",
    "\n",
    "**DataFrame 1: `store_a_sales`**\n",
    "```python\n",
    "store_a_sales = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-01-02'],\n",
    "    'Product': ['A', 'B'],\n",
    "    'Sales': [100, 150]\n",
    "})\n",
    "```\n",
    "\n",
    "**DataFrame 2: `store_b_sales`**\n",
    "```python\n",
    "store_b_sales = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-01-02'],\n",
    "    'Product': ['C', 'A'],\n",
    "    'Sales': [200, 80]\n",
    "})\n",
    "```\n",
    "\n",
    "**DataFrame 3: `store_c_sales`**\n",
    "```python\n",
    "store_c_sales = pd.DataFrame({\n",
    "    'Date': ['2023-01-01'],\n",
    "    'Product': ['B'],\n",
    "    'Sales': [120]\n",
    "})\n",
    "```\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Import Pandas.**\n",
    "2.  Create the three DataFrames as described above.\n",
    "3.  Use `pd.concat()` to combine all three DataFrames into a single DataFrame called `total_sales`.\n",
    "4.  Make sure the final `total_sales` DataFrame has a clean, continuous index (from 0 to 4).\n",
    "5.  Print the final `total_sales` DataFrame.\n",
    "\n",
    "This task will solidify your understanding of how to stack multiple data sources into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fad730a-44f5-4deb-a8f7-f356ad1d8d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Product  Sales\n",
      "0  2023-01-01       A    100\n",
      "1  2023-01-02       B    150\n",
      "2  2023-01-01       C    200\n",
      "3  2023-01-02       A     80\n",
      "4  2023-01-01       B    120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "store_a_sales = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-01-02'],\n",
    "    'Product': ['A', 'B'],\n",
    "    'Sales': [100, 150]\n",
    "})\n",
    "\n",
    "store_b_sales = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-01-02'],\n",
    "    'Product': ['C', 'A'],\n",
    "    'Sales': [200, 80]\n",
    "})\n",
    "\n",
    "store_c_sales = pd.DataFrame({\n",
    "    'Date': ['2023-01-01'],\n",
    "    'Product': ['B'],\n",
    "    'Sales': [120]\n",
    "})\n",
    "\n",
    "total_sales = pd.concat([store_a_sales, store_b_sales, store_c_sales], ignore_index=True)\n",
    "print(total_sales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
